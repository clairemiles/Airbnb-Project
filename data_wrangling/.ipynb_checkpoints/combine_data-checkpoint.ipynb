{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling: Combining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the work completed in the Data Wrangling: Web Scraping notebook. Now that the raw files have been created and stored, the data will be further consolidated into their listings and reviews categories in the form of the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Consolidate the Data\n",
    "Because there are hundreds of raw files to process with hundreds of thousands of lines of data, it helps to create functions that will do the heavy lifting for us. This heavy lifing includes:\n",
    "1. Checking if the consolidated csv files we want already exist on the computer: **consolidate_data**.\n",
    "2. Concatenating data of the same city and category (listings or reviews) together: **combine_listings, combine_reviews, and concat_files**.\n",
    "3. Saving the concatenated data as a csv file: **export_csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_data(city, directory, target):\n",
    "    \"\"\" Checks if the csv file for either listings\n",
    "        or reviews data has been created for the designated\n",
    "        city in the target folder.\n",
    "        If the file has not been created, run the combine_listings\n",
    "        or combine_reviews function for that city, and then create\n",
    "        the csv file for that city.\n",
    "    \"\"\"\n",
    "    \n",
    "    filename = city + '_listings.csv'\n",
    "    # if listings file for this city doesn't already exist, create listings_df and save as csv\n",
    "    if(not os.path.isfile(target + filename)):\n",
    "        listings_df = combine_listings(city, directory)\n",
    "        export_csv(city, filename, listings_df, target)\n",
    "    \n",
    "    filename = city + '_reviews.csv'\n",
    "    # if reviews file for this city doesn't already exist, create reviews_df and save as csv\n",
    "    if(not os.path.isfile(target + filename)):\n",
    "        reviews_df = combine_reviews(city, directory)\n",
    "        export_csv(city, filename, reviews_df, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FUNCTION FOR LISTINGS #### \n",
    "def combine_listings(city, directory):\n",
    "    \"\"\" Goes through files in the directory and checks for the\n",
    "        designated city listings files. Appends the names of the\n",
    "        listings files of that city to a list, and passes the list\n",
    "        and the directory name to the concat_files function.\n",
    "    \"\"\"\n",
    "    \n",
    "    target_files = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        # check if file from the target city and is listings data\n",
    "        if city in file and 'listings' in file:\n",
    "            # add to list of target files\n",
    "            target_files.append(file)\n",
    "            \n",
    "    # concatenate files in list\n",
    "    return concat_files(target_files, directory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FUNCTION FOR REVIEWS #### \n",
    "def combine_reviews(city, directory):\n",
    "    \"\"\" Goes through files in the directory and checks for the\n",
    "        designated city reviews files. Add the names of the\n",
    "        reviews files of that city to a list, and passes the list\n",
    "        and the directory name to the concat_files function.\n",
    "    \"\"\"\n",
    "    target_files = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        # check if file from the target city and is listings data\n",
    "        if city in file and 'reviews' in file:\n",
    "            # add to list of target files\n",
    "            target_files.append(file)\n",
    "            \n",
    "    # concatenate files in list\n",
    "    return concat_files(target_files, directory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files(file_list, directory):\n",
    "    \"\"\"Creates a pandas dataframe for each file name in the \n",
    "       list of files, then adds the date recorded as a column\n",
    "       in that dataframe (taken from the file name). Appends\n",
    "       the dataframe to a list of dataframes. After all files\n",
    "       in the list have been converted to pandas dataframes,\n",
    "       concatenate the dataframes together, drop duplicates,\n",
    "       and reset the dataframe index.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_files = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        # make into a pandas dataframe\n",
    "        df = pd.read_csv(directory + file)\n",
    "        \n",
    "        # add column of the date\n",
    "        df['date_recorded'] = file.split('_')[1]\n",
    "        \n",
    "        # append to a list of dataframes\n",
    "        all_files.append(df)\n",
    "    \n",
    "    # append dataframes together along x-axis\n",
    "    concat_all = pd.concat(all_files)\n",
    "    # get rid of duplicates\n",
    "    unique_all = concat_all.drop_duplicates()\n",
    "    # reset index\n",
    "    unique_all.reset_index(drop=True, inplace=True)\n",
    "    return unique_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv(city, filename, df, target):\n",
    "    \"\"\" If the desired csv file does not exist in the current\n",
    "        working directory, convert the dataframe to a csv file\n",
    "        and move the the desired folder in the target directory.\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd() + '/' + filename\n",
    "    # export listings dataframe to csv if file doesn't already exist\n",
    "    if(not os.path.isfile(current_dir)):\n",
    "        df.to_csv(filename, index=False)\n",
    "        # move csv to target directory\n",
    "        shutil.move(current_dir, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code uses the above functions on the project data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the directory and target folder\n",
    "directory = '/Users/limesncoconuts2/springboard_data/data_capstone_one/web_scraped/'\n",
    "target = '/Users/limesncoconuts2/springboard_data/data_capstone_one/csv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84  cities\n",
      "['amsterdam', 'antwerp', 'asheville', 'athens', 'austin', 'barcelona', 'barossa-valley', 'barwon-south-west-vic', 'beijing', 'bergamo', 'berlin', 'bologna', 'bordeaux', 'boston', 'bristol', 'brussels', 'cambridge', 'cape-town', 'chicago', 'clark-county-nv', 'columbus', 'copenhagen', 'denver', 'dublin', 'edinburgh', 'euskadi', 'florence', 'geneva', 'ghent', 'girona', 'greater-manchester', 'hawaii', 'hong-kong', 'istanbul', 'lisbon', 'london', 'los-angeles', 'lyon', 'madrid', 'malaga', 'mallorca', 'manchester', 'melbourne', 'menorca', 'milan', 'montreal', 'naples', 'nashville', 'new-orleans', 'new-york-city', 'northern-rivers', 'oakland', 'oslo', 'pacific-grove', 'paris', 'portland', 'porto', 'prague', 'puglia', 'quebec-city', 'rhode-island', 'rio-de-janeiro', 'rome', 'salem-or', 'san-diego', 'san-francisco', 'santa-clara-county', 'santa-cruz-county', 'seattle', 'sevilla', 'sicily', 'stockholm', 'sydney', 'taipei', 'tasmania', 'toronto', 'trentino', 'twin-cities-msa', 'vancouver', 'venice', 'victoria', 'vienna', 'washington-dc', 'western-australia']\n"
     ]
    }
   ],
   "source": [
    "# get list of unique cities in alphabetical order\n",
    "unique_cities = []\n",
    "for file in os.listdir(directory):\n",
    "    unique_cities.append(file.split('_')[0])\n",
    "unique_cities = list(set(unique_cities))\n",
    "unique_cities.sort()\n",
    "print(len(unique_cities), ' cities')\n",
    "print(unique_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function on the list of cities \n",
    "for city in unique_cities:\n",
    "    # if both files haven't been created, continue to create the consolidated csv files for that city\n",
    "    if(not os.path.isfile(target + city + '_listings.csv') or not os.path.isfile(target + city + '_reviews.csv')):\n",
    "        consolidate_data(city, directory, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
