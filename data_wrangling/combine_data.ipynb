{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling: Combining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the work completed in the Data Wrangling: Web Scraping notebook. Now that the raw files have been created and stored, the data will be further consolidated into their listings and reviews categories in the form of the pandas dataframe.\n",
    "\n",
    "Since I have a very large dataset scraped from the original webpage, I've decided to use a subset of the data for the present analysis. I will move forward with analysis on the files from Los Angeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Consolidate the Data\n",
    "Because there are several raw files to process with hundreds of thousands of lines of data, it helps to create functions that will do the heavy lifting for us. This heavy lifing includes:\n",
    "1. Checking if the consolidated csv files we want already exist on the computer: **consolidate_data**.\n",
    "2. Concatenating data of the same city and category (listings or reviews) together: **combine_listings, combine_reviews, and concat_files**.\n",
    "3. Saving the concatenated data as a csv file: **export_csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_data(city, directory, destination):\n",
    "    \"\"\" Checks if the csv file for either listings\n",
    "        or reviews data has been created for the designated\n",
    "        city in the destination folder.\n",
    "        If the file has not been created, run the combine_listings\n",
    "        or combine_reviews function for that city, and then create\n",
    "        the csv file for that city.\n",
    "    \"\"\"\n",
    "    \n",
    "    filename = city + '_listings.csv'\n",
    "    # if listings file for this city doesn't already exist, create listings_df and save as csv\n",
    "    if(not os.path.isfile(destination + filename)):\n",
    "        listings_df = combine_listings(city, directory)\n",
    "        export_csv(city, filename, listings_df, destination)\n",
    "    \n",
    "    filename = city + '_reviews.csv'\n",
    "    # if reviews file for this city doesn't already exist, create reviews_df and save as csv\n",
    "    if(not os.path.isfile(destination + filename)):\n",
    "        reviews_df = combine_reviews(city, directory)\n",
    "        export_csv(city, filename, reviews_df, destination)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FUNCTION FOR LISTINGS #### \n",
    "def combine_listings(city, directory):\n",
    "    \"\"\" Goes through files in the directory and checks for the\n",
    "        designated city listings files. Appends the names of the\n",
    "        listings files of that city to a list, and passes the list\n",
    "        and the directory name to the concat_files function.\n",
    "    \"\"\"\n",
    "    target_files = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        # check if file from the target city and is listings data\n",
    "        if city in file and 'listings' in file:\n",
    "            # add to list of target files\n",
    "            target_files.append(file)\n",
    "            \n",
    "    # concatenate files in list\n",
    "    return concat_files(target_files, directory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FUNCTION FOR REVIEWS #### \n",
    "def combine_reviews(city, directory):\n",
    "    \"\"\" Goes through files in the directory and checks for the\n",
    "        designated city reviews files. Add the names of the\n",
    "        reviews files of that city to a list, and passes the list\n",
    "        and the directory name to the concat_files function.\n",
    "    \"\"\"\n",
    "    target_files = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        # check if file from the target city and is listings data\n",
    "        if city in file and 'reviews' in file:\n",
    "            # add to list of target files\n",
    "            target_files.append(file)\n",
    "            \n",
    "    # concatenate files in list\n",
    "    return concat_files(target_files, directory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files(file_list, directory):\n",
    "    \"\"\"Creates a pandas dataframe for each file name in the \n",
    "       list of files, then adds the date recorded as a column\n",
    "       in that dataframe (taken from the file name). Appends\n",
    "       the dataframe to a list of dataframes. After all files\n",
    "       in the list have been converted to pandas dataframes,\n",
    "       concatenate the dataframes together, drop duplicates (ignoring the date_recorded column),\n",
    "       and reset the dataframe index.\n",
    "    \"\"\"\n",
    "    ### ADD THINGS TO MAKE DATAFRAMES MORE EFFICIENT ###\n",
    "    # change datatypes to be more efficient\n",
    "    all_dfs = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        # make into a pandas dataframe\n",
    "        df = pd.read_csv(directory + file)\n",
    "        \n",
    "        # add column of the date\n",
    "        df['date_recorded'] = file.split('_')[1]\n",
    "        \n",
    "        # get rid of duplicates, ignoring new date column\n",
    "        df = df.drop_duplicates(df.columns.difference(['date_recorded']))\n",
    "        \n",
    "        # append to a list of dataframes\n",
    "        all_dfs.append(df)\n",
    "    \n",
    "    # append dataframes together along x-axis\n",
    "    concat_all = pd.concat(all_dfs)\n",
    "\n",
    "    # reset index\n",
    "    concat_all.reset_index(drop=True, inplace=True)\n",
    "    return concat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv(city, filename, df, destination):\n",
    "    \"\"\" If the desired csv file does not exist in the current\n",
    "        working directory, convert the dataframe to a csv file\n",
    "        and move the the desired folder in the destination directory.\n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd() + '/' + filename\n",
    "    # export listings dataframe to csv if file doesn't already exist\n",
    "    if(not os.path.isfile(current_dir)):\n",
    "        df.to_csv(filename, index=False)\n",
    "        # move csv to destination directory\n",
    "        shutil.move(os.path.join(current_dir), os.path.join(destination, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code uses the above functions on the Los Angeles data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the directory and destination folder\n",
    "directory = '/Users/limesncoconuts2/springboard_data/data_capstone_one/web_scraped/'\n",
    "destination = '/Users/limesncoconuts2/springboard_data/data_capstone_one/los_angeles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 15.199235884348552 minutes\n"
     ]
    }
   ],
   "source": [
    "# run function on the list of cities \n",
    "start_time = time.time() # timestamp\n",
    "city = 'los-angeles'\n",
    "\n",
    "# if both files haven't been created, continue to create the consolidated csv files for that city\n",
    "if(not os.path.isfile(destination + city + '_listings.csv') or not os.path.isfile(destination + city + '_reviews.csv')):\n",
    "    consolidate_data(city, directory, destination)\n",
    "\n",
    "time_to_run = (time.time() - start_time)/60 # timestamp, calculate function time\n",
    "print('Time:',time_to_run, 'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating functions will allow us to re-use this code if we wanted to train an algorithm on files for all cities, or a larger subset of the cities. We would simply need to create a list of all the unique cities represented in the files that we scraped from the internet, and then loop through those cities in the consolidate_data function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
