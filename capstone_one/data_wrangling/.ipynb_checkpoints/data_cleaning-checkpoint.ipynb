{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been sourced and consolidated into separate csv files for each city, I will further clean and  the data using pandas dataframes. I will also conduct some preliminary data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish the directory\n",
    "directory = '/Users/limesncoconuts2/springboard_data/data_capstone_one/csv/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate all files into a single dataframe\n",
    "It will be easier to explore the Airbnb listings and reviews data if the data for all of the cities is consolidated into one dataframe for listings and another for reviews.\n",
    "\n",
    "In order to keep the data tidy and to preserve vital information about the data, I will add a column called 'city_name' each time a csv file is imported to preserve the information about which city the data corresponds to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code idea on one dataframe\n",
    "Using the time package, I tested importing the dataframe on the largest listings and reviews files, which are for Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0,2,4,5,7,9,10,11,14,16,18,21,24,25,27,28,29,30,35,40,43,44,45,46,47,48,50,54,59,60,63,67,72,73,85,86,92,93,95,96) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# time test of largest listings dataframe\n",
    "start_time = time.time()\n",
    "df = pd.read_csv(directory + 'paris_listings.csv')\n",
    "time_to_run = time.time() - start_time\n",
    "print('Time:',time_to_run)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time test of largest reviews dataframe\n",
    "start_time = time.time()\n",
    "df = pd.read_csv(directory + 'paris_review.csv')\n",
    "time_to_run = time.time() - start_time\n",
    "print('Time:',time_to_run)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import listings files, add a city column for each file\n",
    "listings_list = []\n",
    "for file in os.listdir(directory):\n",
    "    df = pd.read_csv(directory + file)\n",
    "    df['city_name'] = file.split('_')[0]\n",
    "    listings_list.append(df)\n",
    "    # concat files in list\n",
    "    listings = pd.concat(listings_list)\n",
    "    # reset index\n",
    "    listings.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import reviews files, add a city column for each file\n",
    "reviews_list = []\n",
    "for file in os.listdir(directory):\n",
    "    df = pd.read_csv(directory + file)\n",
    "    df['city_name'] = file.split('_')[0]\n",
    "    reviews_list.append(df)\n",
    "    # concat files in list\n",
    "    reviews = pd.concat(reviews_list)\n",
    "    # reset index\n",
    "    reviews.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Due Diligence\n",
    "Because of the sheer size of the data, I will start out by going through a checklist of standard aspects to investigate in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Untidy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statistics\n",
    "(.describe())/frequency counts/plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Stuff**\n",
    "- need to process columns\n",
    "- column types can signal unexpected data values\n",
    "- summary statistics\n",
    "- merging/concatenating data\n",
    "- assert statements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
