{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I am using data from http://insideairbnb.com, which stores its datasets by city. I'm focusing on the listings.csv and reviews.csv files for each city, accessing them by scraping the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import relevant packages**\n",
    "\n",
    "The requests and BeautifulSoup packages allow me to scrape the url in question and extract the relevant information I need (the csv file names and content).\n",
    "\n",
    "The os and shutil packages allow me to check if the files I'm writing from the website are already in the directory (since there are so many files and the session would time out, I had to run the script a few times) and put the files in a new folder once they were all created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Request data**\n",
    "\n",
    "I used requests to capture the response from the url where all of the csv files are hosted: http://insideairbnb.com/get-the-data.html. After catching that response in a variable, I read the text into another variable. I turned the text variable into a BeautifulSoup object using the BeautifulSoup function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch the response from the url and convert to BeautifulSoup object\n",
    "url = 'http://insideairbnb.com/get-the-data.html'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Make list of target filenames**\n",
    "\n",
    "Using BeautifulSoup and requests, I extracted the text from the HTML anchor tags, where the csv links are found in the ‘href’ attribute of the tag. I stored the names of the csv urls in a list. Since the links led to gzipped (.csv.gz) files, I called the list _zipped_\\__links_.\n",
    "\n",
    "Since I only wanted the raw data about listings and reviews, I used an if statement to choose only those files. Some of the files, however, are actually summary statistics of listings and reviews data that are optimized for visualization. I excluded the 'visualization' keyword in these file names using the if statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all href links in <a> tags that contain 'listings.csv' or 'reviews.csv' (but not the visualizations) and store in a list\n",
    "# all links are in gzip files\n",
    "zipped_links = []\n",
    "for link in soup.find_all('a'):\n",
    "    link_url = link.get('href')\n",
    "    # only choose csv files that are listings or review data, exclude visualizations\n",
    "    if (link_url is not None) and ('listings.csv' in link_url or 'reviews.csv' in link_url) and ('visualisations' not in link_url):\n",
    "        zipped_links.append(link_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Write csv.gz files locally**\n",
    "\n",
    "I wrote the information from the online gzipped csv files to files on my local server. Using a context manager, I looped through _zipped_\\__links_ to create custom filenames for each list item and write to that file. The filename includes the city name, the date the data was collected, and the information category (listings or reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to write information from the links to their own files\n",
    "def write_files(ls, directory):\n",
    "    for link in ls:\n",
    "        file_url_split = link.split('/')\n",
    "        filename = file_url_split[-4] + '_' + file_url_split[-3] + '_' + file_url_split[-1]\n",
    "        # if the file doesn't exist in our directory, write to the file\n",
    "        if(not os.path.isfile(directory + filename)):\n",
    "            with open(directory + filename, \"wb\") as f:\n",
    "                r = requests.get(link)\n",
    "                f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL FILES WRITTEN!\n"
     ]
    }
   ],
   "source": [
    "# implement function in script\n",
    "directory = '/Users/limesncoconuts2/springboard_data/data_capstone_one/web_scraped'\n",
    "\n",
    "# remove files that are 0 bytes (program timed out while they were being written previously)\n",
    "for file in os.listdir(directory):\n",
    "    if os.path.getsize(directory + file) == 0:\n",
    "        os.remove(directory + file)\n",
    "\n",
    "# check if all files have been written\n",
    "# if not, run writing function again\n",
    "# if so, print affirmative statement\n",
    "if len(zipped_links) != len(os.listdir(directory)):\n",
    "    write_files(zipped_links, directory)\n",
    "print('ALL FILES WRITTEN!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some links to files on the website are broken, so we have to exclude these from our data\n",
    "# remove files that are of less than 1kb (not actually csv.gz files because of broken url)\n",
    "for file in os.listdir(directory):\n",
    "    if os.path.getsize(directory + file) < 1000:\n",
    "        os.remove(directory + file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
